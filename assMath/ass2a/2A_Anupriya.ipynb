{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment 2A**\n"
      ],
      "metadata": {
        "id": "Nn59ntocyMfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __Question 1__\n",
        "\n",
        "The posterior, likelihood and prior are related according to,\n",
        "\n",
        "$ P(\\theta | y) = \\frac{P(y | \\theta) P(\\theta)}{P(y)} $\n",
        "\n",
        "Let A denote that Alice is working, B denote that Bob is working and $d$ denote the data that the boss has collected.\n",
        "\n",
        "$ \\lambda_a (=10) $ is the average number of tickets Alice collects and $ \\lambda_b (=15) $ is the average number of tickets Bob collects and note that the number of tickets $ X$ that they collect is modelled by a possion distribution,\n",
        "\n",
        "$ P(X=x)= \\frac{\\lambda ^x e^{-\\lambda}}{x!}$\n",
        "\n",
        "Now, odds that Alice is working in place of Bob = $O(A)=\\frac{P(A)}{P(B)}=\\frac{1}{10}$\n",
        "\n",
        "Thus, $P(A)=\\frac{1}{11}$ and $P(B)=\\frac{10}{11}$\n",
        "\n",
        "Further, we need the posterior odds that Alice is filling in for Bob, that is,\n",
        "\n",
        "$O($Alice works|data$)=\\frac{P(Alice\\;works|data)}{P(Bob\\;works|data)}=\\frac{P(A|d)}{P(B|d)}$\n",
        "\n",
        "Here,\n",
        "\n",
        "$P(A|d)=\\frac{P(d|A).P(A)}{P(d)}=\\frac{\\prod\\limits_{i=1}^{5}\\frac{\\lambda_a ^{x_i} e^{-\\lambda}}{x_i!}.P(A)}{P(d)} = \\frac{\\frac{10^{12+10+11+4+11} e^{-50}}{12! 10! 11! 4! 11!}}{P(d)}.\\frac{1}{11}$\n",
        "\n",
        "$P(B|d)=\\frac{P(d|B). P(B)}{P(d)}= \\frac{\\prod\\limits_{i=1}^{5}\\frac{\\lambda_b ^{x_i} e^{-\\lambda}}{x_i!}.P(B)}{P(d)} =\\frac{ \\frac{15^{12+10+11+4+11} e^{-75}}{12! 10! 11! 4! 11!}}{P(d)}. \\frac{10}{11}$\n",
        "\n",
        "$O($Alice works|data$)=\\frac{P(d|A)}{P(d|B)}\\frac{P(A)}{P(B)} = (\\frac{10}{15})^{48}e^{25} (\\frac{1}{10}) ≈ 25.409$"
      ],
      "metadata": {
        "id": "mFqla4Vxeys9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __Question 2__\n",
        "\n",
        "#### __Part (a)__\n",
        "According to the question,\n",
        "\n",
        "$f(\\theta)\\sim N(5,9) \\implies P(\\theta)= \\frac{1}{3\\sqrt{2\\pi}}e^{\\frac{{-(\\theta-5)^2}}{18}}$ ...prior\n",
        "\n",
        "$f(x|\\theta) \\sim N(\\theta,4) \\implies P(x|\\theta)=\\frac{1}{2\\sqrt{2\\pi}}e^{\\frac{{-(x-\\theta)^2}}{8}}$ ...likelihood\n",
        "\n",
        "Where x=6, hence,\n",
        "\n",
        "$P(x=6|\\theta)= \\frac{1}{2\\sqrt{2\\pi}}e^{\\frac{{-(6-\\theta)^2}}{8}} $\n",
        "\n",
        "Then from the usual Bayesian update table,\n",
        "\n",
        "Posterior, i.e., $f(\\theta|x)=f(x|\\theta).f(\\theta)=\\frac{1}{12\\pi}(e^{-\\frac{(\\theta-5)^2}{18}}.e^{\\frac{-(6-\\theta)^2}{8}}) = \\frac{1}{12\\pi}(e^{{\\frac{-(\\theta-5)^2}{18}}-{\\frac{(6-\\theta)^2}{8}}})$\n",
        "\n",
        "We need to modify the posterior to obtain what the mean and variance will be for the Normal distribution,\n",
        "\n",
        "$\\frac{(\\theta-5)^2}{18}+ \\frac{(6-\\theta)^2}{8}= \\frac{1}{2}(\\frac{\\theta^2-10\\theta+25}{9} + \\frac{\\theta^2-12\\theta+36}{4})$\n",
        "$= \\frac{1}{2}(\\frac{13\\theta^2-148\\theta+424}{36})$\n",
        "$=\\frac{1}{2}(13\\frac{(\\theta^2-\\frac{74}{13})^2+36}{36}) $\n",
        "$=\\frac{1}{2}(\\frac{(\\theta^2-\\frac{74}{13})^2+ 36}{\\frac{36}{13}})$\n",
        "\n",
        "Hence,\n",
        "$f(\\theta|x) \\propto e^{-\\frac{1}{2}(\\frac{(\\theta^2-\\frac{74}{13})^2}{\\frac{36}{13}})}$\n",
        "\n",
        "$\\therefore f(\\theta|x)\\sim N(\\frac{74}{13},\\frac{36}{13})$"
      ],
      "metadata": {
        "id": "-O6Dk3i4ywGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Part (b)__\n",
        "\n",
        "Using the formulas given to us,\n",
        "\n",
        "$\\sigma^2 = 4$\n",
        "\n",
        "$\\mu_{prior}=5$\n",
        "\n",
        "$\\sigma_{prior}^2=9$\n",
        "\n",
        "$\\bar{x}=6$\n",
        "\n",
        "$n=4$\n",
        "\n",
        "We get,\n",
        "\n",
        "$a=\\frac{1}{\\sigma_{prior}^2}=\\frac{1}{9}$\n",
        "\n",
        "$b=\\frac{n}{\\sigma^2}=1$\n",
        "\n",
        "$\\mu_{post}=\\frac{{a\\mu_{prior}+b\\bar{x}}}{a+b}=\\frac{\\frac{5}{9}+6}{\\frac{1}{9}+1}=5.9$\n",
        "\n",
        "$\\sigma_{post}^2=\\frac{1}{a+b}=\\frac{1}{\\frac{1}{9}+1}=0.9$\n",
        "\n",
        "Thus, the posterior on theta, $f(\\theta|x)\\sim N(5.9,0.9)$\n",
        "\n",
        "The plot showing the prior and posterior is-"
      ],
      "metadata": {
        "id": "pJyTh7MjFFL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "x = np.arange(-10, 30, 0.0001)\n",
        "plt.plot(x, norm.pdf(x, 5, 3), label='μ: 5, σ^2: 9')\n",
        "plt.plot(x, norm.pdf(x, 5.9, 0.948683298), label='μ:5.9, σ^2: 0.9')\n",
        "\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ZX6hDNLZI-7n",
        "outputId": "a5595ae1-6759-4fe3-c96a-466e4857f80d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff0fd8a36a0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that compared to the prior, the posterior has very small variance $(=0.9)$ meaning more accuracy and we see that the posterior has a mean $(=5.9)$ which is very close to $\\bar{x}=6$. Thus we can say with more certainty that that value we receive is closer to $6$."
      ],
      "metadata": {
        "id": "xiVBxQO9LB0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Part (c)__\n",
        "\n",
        "$a=\\frac{1}{\\sigma_{prior}^2}$\n",
        "\n",
        "$b=\\frac{n}{\\sigma^2}$\n",
        "\n",
        "$\\mu_{post}=\\frac{{a\\mu_{prior}+b\\bar{x}}}{a+b}$\n",
        "\n",
        "$\\sigma_{post}^2=\\frac{1}{a+b}$\n",
        "\n",
        "If there are more signals being received then $n$ increases. If $n$ increases, so does $b$. Clearly, then $\\sigma_{post}^2$ decreases, meaning less variance and more accuracy.\n",
        "We can say about $\\mu_{post}$ that the weightage of $\\bar{x}$ increases as $b$ increases however no change occurs in $a$ or $\\mu_{prior}$. So, there is just more weightage of $\\bar{x}$ in the value of posterior mean."
      ],
      "metadata": {
        "id": "uBe6gIGoMAaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### __Part (d)__\n",
        "\n",
        "We have been given that IQ follows a distribution $N(100,152)$, so this is the prior that we already have with us, where $\\theta$ is the unknown value of the true IQ such that,\n",
        "\n",
        "$ f(\\theta)\\sim N(100,152)$\n",
        "\n",
        "Next, if a person is tested multiple times, their measured IQ (say $x$) differs from their true IQ ($\\theta$) according to $N(0,102)$. As the mean is 0, the difference in the mean of the measured IQ and the true IQ (which is the true IQ itself which is no more unknown to us) is 0, or the measured IQ has a mean equal to the true value itself,\n",
        "So, the likelihood of some value of IQ given some true value of IQ $\\theta$,\n",
        "\n",
        "$f(x|\\theta) \\sim N(\\theta,102)$\n",
        "\n",
        "$ (i)$ Randall Vard scored 80, so the measured IQ is 80. We can say that the expected value of his true IQ will be obtained from the posterior, the probability of his true IQ given his measured IQ. So, to get the expected value (or mean), we use,\n",
        "\n",
        "$a=\\frac{1}{\\sigma_{prior}^2}$\n",
        "\n",
        "$b=\\frac{n}{\\sigma^2}$\n",
        "\n",
        "$\\mu_{post}=\\frac{{a\\mu_{prior}+b\\bar{x}}}{a+b}$\n",
        "\n",
        "Where,\n",
        "\n",
        "$\\sigma_{prior}^2=152$\n",
        "\n",
        "$\\mu_{prior}=100$\n",
        "\n",
        "$n=1$ and\n",
        "\n",
        "$\\sigma^2=102$ (known variance)\n",
        "\n",
        "Using these,\n",
        "\n",
        "$a=\\frac{1}{152}$\n",
        "\n",
        "$b=\\frac{1}{102}$\n",
        "\n",
        "Here, $\\bar{x}=x=80$ thus,\n",
        "\n",
        "$\\mu_{post}= \\frac{\\frac{1}{152}.100+\\frac{1}{102}.80}{\\frac{1}{152}+\\frac{1}{102}} = 88.031$\n",
        "\n",
        "$(ii)$ Here, for Mary I. Taft, $\\bar{x}=x=150$ thus,\n",
        "\n",
        "$\\mu_{post}= \\frac{\\frac{1}{152}.100+\\frac{1}{102}.150}{\\frac{1}{152}+\\frac{1}{102}} = 129.914$\n"
      ],
      "metadata": {
        "id": "el5ZV7bdPr64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###__Question 3__\n",
        "We already know how we can use MLE to estimate the unknown mean and variance of a given dataset that follows gaussian distribution.\n",
        "\n",
        "So, let $\\theta$ denote the set of unknown parameters $\\mu$ and $\\sigma$,\n",
        "\n",
        "$\\theta = \\{\\mu,\\sigma\\}$\n",
        "\n",
        "The goal of MLE is the maximize the probability (actually probability density since the data is continuous) of the data given $\\theta$. Hence we want to maximise the following function -\n",
        "\n",
        "$f\\{x_1,x_2....x_n|\\theta\\}$\n",
        "\n",
        "where, of course,\n",
        "\n",
        "$f(x_i|\\theta)= \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{\\frac{{-(x_i-\\mu)^2}}{2\\sigma^2}}$\n",
        "\n",
        "Thus we aim the find the optimal $\\theta$ for which $f$ is maximised,\n",
        "\n",
        "$\\hat{\\theta}_{MLE}=argmax_{\\theta}\\prod\\limits_{i}^{n}f(x_i|\\theta)$\n",
        "\n",
        "To simplify calculation (taking derivative to maximise $f$), we use logarithmic function. Then by simple algebra, our equatio above changes to,\n",
        "\n",
        "$\\hat{\\theta}_{MLE}=argmax_{\\theta}\\sum_\\limits{i}^{n}ln(f(x_i|\\theta))$\n",
        "\n",
        "From MAN-006, we have already calculated the parameters estimated by MLE by taking the derivative of the log of the probability density f with respect to each unknown parameter and equating that to zero, and they turn out to be,\n",
        "\n",
        "$\\hat{\\mu}_{MLE}=\\frac{1}{n}\\sum\\limits_{i}^{n}x_i$\n",
        "\n",
        "Which is simply the sample mean, and,\n",
        "\n",
        "$\\hat{\\sigma}^2_{MLE}=\\frac{1}{n}\\sum\\limits_{i}^{n}(x_i-\\mu)^2$\n",
        "\n",
        "Which is the sample variance.\n",
        "\n",
        "Now, our next goal is to use python to estimate this. So, all we should have to do is give a dataset that follows gaussian distibution, define the Maximum likelihood function (the logarithmic form) and then maximize it using python to obtain the required estimates.\n",
        "\n",
        "A small research on the internet revealed that the easiest way to maximise the function would be using Scipy. Instead of trying to maximize the log-likelihood funciton, the standard way seems to be to minimize the negative log-likelihood function."
      ],
      "metadata": {
        "id": "Nrayn0lxLFd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first let's generate a dataset following a gaussian distribution using numpy\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import norm\n",
        "\n",
        "truemean = 100\n",
        "truevariance=152\n",
        "truesd=np.sqrt(truevariance)\n",
        "n=1000\n",
        "dataset = np.random.normal(truemean,truesd, size=n)\n",
        "print(\"True Mean:\", truemean)\n",
        "print(\"True Variance:\",truevariance )\n",
        "\n",
        "#print(dataset)\n",
        "\n",
        "# now we must define the negative log-likelihood function\n",
        "\"\"\"\n",
        "def likelihood(parameters,dataset):\n",
        "    mean = parameters[0]\n",
        "    sd=parameters[1]\n",
        "    sum=0\n",
        "    for i in dataset:\n",
        "      data=i\n",
        "      pdf = norm.pdf(data , loc = mean , scale = sd )\n",
        "      LL=np.log(pdf)\n",
        "      sum+=LL\n",
        "    negativeLL= -1*sum\n",
        "    return negativeLL\n",
        "\"\"\"\n",
        "\n",
        "def likelihood(parameters, dataset):\n",
        "    mean, variance = parameters\n",
        "    sum=0\n",
        "    N = len(dataset)\n",
        "    for i in dataset:\n",
        "      sum+=(i-mean)**2\n",
        "    LL = - N/2 * np.log(variance) - 1/(2*variance) * sum\n",
        "    return -LL\n",
        "\n",
        "# Scipy optimization algorithms require a intial parameter value, closer to the true value, faster the optimization algorithm\n",
        "# we will set the initial parameters to the standard normal distribution values since we don't have a prior\n",
        "initialparameters=[0,1]\n",
        "\n",
        "# now we can use Scipy to minimize the negative log-likelihood function\n",
        "result= minimize(likelihood, initialparameters, args=(dataset,),method='L-BFGS-B')\n",
        "estimatedmean,estimatedsd=result.x\n",
        "print(\"Estimated Mean:\", estimatedmean)\n",
        "print(\"Estimated Standard deviation:\",estimatedsd )\n",
        "\n",
        "# at first I was using scipy to define the normal pdf but....\n",
        "# this was not working on high values of mean and standard deviation (bigger values are giving RuntimeWarning: divide by zero encountered in log )\n",
        "# I tried searching about this error a lot but didn't find anything very helpful then I switched to defining log-likelihood function from scratch and it worked!\n",
        "# further it important to specify the method of optimization as L-BFGS-B otherwise we keep getting \"invalid value encountered in log\" error. No idea what's happening!\n",
        "# L-BFGS-B is a variation of the standard BFGS algorithm which is used when we have limited memory and our variables have some bound, its also much faster\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5Z7s0BXWVOf",
        "outputId": "089816a0-7b42-43fb-8b10-c5309fd03eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Mean: 100\n",
            "True Variance: 152\n",
            "Estimated Mean: 99.46425688255611\n",
            "Estimated Standard deviation: 167.37918914364832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###__Question 4__\n",
        "\n",
        "Firstly, let's break down the problem and understand what is asked of us.\n",
        "\n",
        "So, we have a dataset of binary classification, which means our data has outcomes 0 or 1 only. We want to use logistic regression model for the classification and we're been given that the model parameters (conventionally taken as __w__) has a gaussian prior.\n",
        "\n",
        "Now, we want to formulate the likelihood, the prior and the posterior and then find the MAP estimate of the model parameters.\n",
        "\n",
        "After a bit of revision, we recall that in logistic regression we use the logistic function to define the likelihood function such that it is exactly of the form,\n",
        "\n",
        "$P(y|\\textbf{x})=\\frac{1}{1+e^{(\\textbf{w}^{\\textbf{T}}\\textbf{x}+b)}}$\n",
        "\n",
        "Where, $\\textbf{w}$ is the vector of parameters.\n",
        "\n",
        "We might make assumptions on the posterior, that is,\n",
        "\n",
        "$P(x|y)$ but it doesn't matter since here we'll be directly using MAP to estimate the parameters.\n",
        "\n",
        "In MAP, our goal is to maximise the posterior (unlike MLE where we maximise the likelihood function). As we already know,\n",
        "\n",
        "$P(\\textbf{w}|y)=P(y|\\textbf{x},\\textbf{w}).P(\\textbf{w})$\n",
        "\n",
        "Then, we require the value of $\\textbf{w}$ such that the posterior is maximized.\n",
        "\n",
        "$argmax_{\\textbf{w}}[P(y|\\textbf{w}).P(\\textbf{w})]=argmax_{\\textbf{w}}\\{[ln[P(y|\\textbf{w})]+ ln[P(\\textbf{w})]\\}=argmin_{\\textbf{w}}\\{-[ln[P(y|\\textbf{w})]- ln[P(\\textbf{w})]\\}$\n",
        "\n",
        "Now, we have a binary classification situation, so we have to utilise the fact that Y can only take values 0 and 1. So, only one of the following two terms is going to be non-zero for any given Y,\n",
        "\n",
        "$P(Y=1|X)=\\frac{e^{(\\textbf{w}^{\\textbf{T}}\\textbf{x}+b)}}{1+e^{(\\textbf{w}^{\\textbf{T}}\\textbf{x}+b)}}$\n",
        "\n",
        "And,\n",
        "\n",
        "$P(Y=0|X)=\\frac{1}{1+e^{(\\textbf{w}^{\\textbf{T}}\\textbf{x}+b)}}$\n",
        "\n",
        "So now our goal becomes,\n",
        "\n",
        "$\\hat{\\textbf{w}}_{MAP}=argmax_{\\textbf{w}}\\{[ln[\\prod\\limits_{1}^{n}P(y_i|\\textbf{w})]+ ln[P(\\textbf{w})]\\}$\n",
        "$=argmax_{\\textbf{w}}\\{[\\sum\\limits_{1}^{n}ln[P(y_i|\\textbf{w})]+ ln[P(\\textbf{w})]\\}$\n",
        "\n",
        "Now, according to the above utilisation, we can rewrite $ln[P(y_i|\\textbf{w})]$ as,\n",
        "\n",
        "$=\\sum\\limits_{1}^{n}Y_i.ln[P(Y_i=1|X_i,\\textbf{w})]+(1-Y_i).ln[P(Y_i=0|X_i,\\textbf{w})]$\n",
        "\n",
        "$=\\sum\\limits_{1}^{n}Y_i.ln[\\frac{P(Y_i=1|X_i,\\textbf{w})}{P(Y_i=0|X_i,\\textbf{w})}]+ln[P(Y_i=0|X_i,\\textbf{w})]$\n",
        "\n",
        "$=\\sum\\limits_{1}^{n}\\{Y_i.(\\textbf{w}^{\\textbf{T}}\\textbf{x}_i+b)-ln(1+e^{(\\textbf{w}^{\\textbf{T}}\\textbf{x}_i+b)})\\}$\n",
        "\n",
        "Now, let's come to the fact that our model paramters have a gaussian prior. We will pick the simplest and most convenient zero-mean gaussian with a known variance $\\sigma$ for every parameter.\n",
        "\n",
        "$w_j \\sim N(0,\\sigma^2)$ such that $P(\\textbf{w})=\\prod\\limits_{1}^{n}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\{\\frac{{-w_j}^2}{2\\sigma^2}\\}}$\n",
        "\n",
        "Then, (excluding terms not containing $w_j$,\n",
        "\n",
        "$ln[P(\\textbf{w})]=-\\frac{1}{2\\sigma^2}\\sum\\limits_{1}^{f}{\\textbf{w}_j}^2$\n",
        "\n",
        "Putting everything together, our goal is,\n",
        "\n",
        "$argmin_{\\textbf{w}}\\{-[ln[P(y|\\textbf{w})]- ln[P(\\textbf{w})]\\}=\\frac{1}{2\\sigma^2}\\sum\\limits_{1}^{f}{\\textbf{w}_j}^2 - \\sum\\limits_{1}^{n}\\{Y_i.(\\textbf{w}^\\textbf{T}.x_i+b)-ln(1+e^{(\\textbf{w}^{\\textbf{T}}\\textbf{x}_i+b)})\\}$\n",
        "\n",
        "So far, I believe we can do the same sort of thing we've done in the previous question - define each of the functions and finally use Scipy to minimise this poeterior.\n",
        "\n",
        "We'll have to specify the prior ourselves, so for the sake of simplicity, we can absorb the parameter $b$ into $w$ through an additional constant dimension in $w$ (read in some papers).\n",
        "\n",
        "So, $w$ has multivariate gaussian prior and once again, for simplicity, let's say it has just two dimensions (we'll also include $b$ so the final dimension of $w$ will be. Our goal now looks like minimising the following function,\n",
        "\n",
        "$=\\sum\\limits_{1}^{f}\\frac{1}{2\\sigma^2}{\\textbf{w}_j}^2 -\\sum\\limits_{1}^{n}\\{Y_i.(\\textbf{w}^{\\textbf{T}}\\textbf{x}_i)-ln(1+e^{(\\textbf{w}^{\\textbf{T}}\\textbf{x}_i)})\\}$\n",
        "\n",
        "So, let us begin."
      ],
      "metadata": {
        "id": "ugh_Avb9wn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from scipy.optimize import minimize\n",
        "\"\"\"\n",
        "# first we'll have to generate a dataset, let size be n and number of features (dimension of w) be f. Here we have just one feature\n",
        "n= 1000\n",
        "f= 2+1\n",
        "x, y = make_classification(n_samples=n,n_features=f,n_redundant=0,n_informative=3,n_repeated=0, random_state=444)\n",
        "#print(x.shape)\n",
        "#print(y)\n",
        "\n",
        "# now for w, it's up to us to choose the variance, so let's assume the following\n",
        "pmean=0\n",
        "pvar=5\n",
        "\n",
        "def logprior(w,variance):\n",
        "    logprior = (-1/(2*variance)) * np.sum(np.square(w))\n",
        "    return logprior\n",
        "\"\"\"\n",
        "# first we'll have to generate a dataset, let size be n and number of features (dimension of w -1) be f.\n",
        "n = 1000\n",
        "f = 2\n",
        "priormean=0 # for every parameter\n",
        "priormeanvector = np.zeros(f + 1) # absorbing parameter b\n",
        "priorvariance=5 # for every parameter and they are independent of each other\n",
        "priorcovariance = priorvariance * np.eye(f + 1)  # absorbing parameter b, covariances are all 0, variance of each is equal to priorvariance\n",
        "\n",
        "truew = np.random.multivariate_normal(priormeanvector, priorcovariance)\n",
        "print(\"True parameters used for data generation :\",truew)\n",
        "\n",
        "x = np.hstack((np.ones((n, 1)), np.random.uniform(-1, 1, size=(n, f)))) # bias is fixed at 1, generating random features (n of them, each is a vector with 3 elements, first is 1)\n",
        "z = np.dot(x, truew)\n",
        "p = 1 / (1 + np.exp(-z)) # y=1 has this probability\n",
        "y = np.random.binomial(1, p)\n",
        "\n",
        "def logprior(w,priormean,priorvariance):\n",
        "    LP = (-1/(2*priorvariance)) * np.sum(np.square(w-priormean))\n",
        "    return LP\n",
        "\n",
        "def loglikelihood(w, x, y):\n",
        "  LL=0\n",
        "  count=0\n",
        "  for i in range(n):\n",
        "    z = np.dot(x[i], w)\n",
        "    LL+=y[i]*(x[i].dot(w.T))-np.log(1+np.exp(z))\n",
        "  return LL\n",
        "\n",
        "\n",
        "def neglogposterior(w,priormean,priorvariance,x,y):\n",
        "  return -logprior(w,priormean,priorvariance)-loglikelihood(w,x,y)\n",
        "\n",
        "# like before, we need initial parameters\n",
        "# print(x.shape)\n",
        "initialparameters_w=np.array([2,2,2]) # w is a vector after all\n",
        "\n",
        "result = minimize(neglogposterior, initialparameters_w, args=(priormean,priorvariance,x,y), method='L-BFGS-B')\n",
        "westimate=result.x\n",
        "print(\"MAP Estimated parameters: \",westimate) # it's pretty close\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB60-rFGx5Mi",
        "outputId": "db68ff27-3cdf-4253-f0c9-73a31fbdb19e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True parameters used for data generation : [ 1.20033799 -2.79411209  3.2057022 ]\n",
            "MAP Estimated parameters:  [ 1.24238212 -3.12122653  3.15600055]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __Question 5__\n",
        "\n",
        "We have been asked to find the VC dimension of some concept classes. To make sure I understood this right, VC dimension for a concept class is simply the maximum number of points that the concept class can create a sort of seperation (shatter) for (given each point belongs to one of two classes and taking all $2^n$ possibilites in mind) such that they're classified correctly in their regions and an important thing to note is that all we need is ONE configuration of these points (and the $2^n$ possibilities of class allocation) which can be shattered by the concept class in order to define the VC dimension for it. Meaning, we don't need every configuration (different locations of points, but any one) to be able to define the VC dimension. Also, I haven't gone through the math intensive article yet.\n",
        "\n",
        "#### __(a) Constant Function__\n",
        "Meaning this is a function that can take any constant value. So, from my understanding, given a set of points which are to be classified into two classes, a constant function can only take either \"positive\" or \"negative\" for all the given points. So, if we had 10 points, we could only assign either positive or negative to all the 10 points. If this understanding is correct, then this concept class can only classify 1 point correctly. If we had 2 points then our possibilities (despite their configuration) become,\n",
        "\n",
        "1. +,-\n",
        "\n",
        "2. -,+\n",
        "\n",
        "3. +,+\n",
        "\n",
        "4. -,-\n",
        "\n",
        "And in such a case, assigning only positive or only negative value to both points doesn't satisfy all 4 cases (but only one at a time, either  case 3 or case 4). So, the VC dimension should be 1.\n",
        "\n",
        "#### __(b) Linear Function in d dimensions__\n",
        "A linear function would generate a line in 2 dimensions, a plane in 3 dimensions and so on. From the reading, a linear classifier has cardinality n+1 (given n is the dimension?). Which makes sense for 2 dimensions as the VC dimension there is 3. Again, I have yet to see the mathematical proof for this one. The VC dimension should be d+1.\n",
        "\n",
        "#### __(c) Axis aligned rectangle in 2 dimensions__\n",
        "Here, we are being restricted by \"axis aligned\". If we have 1,2 or 3 points in space, we can just enclose the points of one class within a large enough axis aligned rectangle. If we had 4 points, we can imagine a configuration where two points are lying sort of diagonally to each other but are close enough in any one dimension to allow us to put them in a narrow axis aligned rectangle. So, we'd still be able to enclose 4 points. In case of 5 points however, even if we have 3 points close together, a different combination of classes would cause difficulty like if we had the centre point of one class (+), the two side points of another class (-) and the other two outward points also of the same class as the centre point (+), then a 2-D axis aligned rectangle can't shatter these points. No other configuration works either. So, VC Dimension has to be 4.\n",
        "\n",
        "#### __(d) Intervals__\n",
        "Intervals as in some interval on an infinite line. Now, if we have 3 points, they'd obviously be collinear and then the situation where a negative(or positive) point lies between two positive (or negative) points, would make it impossible to define an interval to shatter them. So, VC dimension can be 2.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ByZ8ATC1re9E"
      }
    }
  ]
}